"""
Flow Companion Evals Dashboard
Separate app for comparing optimization configurations.

Run: streamlit run evals_app.py --server.port 8502
"""

import streamlit as st
import os
import sys
import json
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd

# Add project root to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

st.set_page_config(
    page_title="Flow Companion Evals",
    page_icon="üìä",
    layout="wide"
)

from evals.configs import EVAL_CONFIGS, DEFAULT_SELECTED
from evals.runner import ComparisonRunner
from evals.result import ComparisonRun, TestComparison, ConfigResult
from evals.storage import save_comparison_run, list_comparison_runs, load_comparison_run

# Import coordinator
from agents.coordinator import coordinator

# Color scheme for optimization configs
CONFIG_COLORS = {
    "baseline": "#6b7280",          # Gray
    "compress_results": "#8b5cf6",  # Purple
    "streamlined_prompt": "#3b82f6", # Blue
    "prompt_caching": "#f59e0b",    # Amber
    "all_context": "#10b981",       # Green
}

# Metric explanations for tooltips
METRIC_EXPLANATIONS = {
    "avg_latency": "Average time from query submission to complete response. Includes LLM thinking time, tool execution (MongoDB queries, embeddings), and processing overhead.",
    "avg_tokens_in": "Average input tokens sent to the LLM per query. Includes system prompt, conversation history, and tool results. Lower = faster and cheaper. Compression and streamlined prompts reduce this.",
    "avg_tokens_out": "Average tokens generated by the LLM per response. Measures response verbosity. Generally consistent across optimizations since output content stays the same.",
    "accuracy": "Percentage of tests that produced correct, relevant responses. Measures whether optimizations maintain response quality while improving speed.",
    "pass_rate": "Number of tests that passed vs total tests run. A test passes if it completes without errors and returns an appropriate response for the query type."
}

# Chart explanations for tooltips
CHART_EXPLANATIONS = {
    "waterfall": "Shows average latency for each optimization configuration (LLM queries only - excludes slash commands which bypass the LLM). Compare individual techniques (Compress, Streamlined, Caching) against Baseline, and see the combined effect with 'All Ctx'. The percentage shows reduction from Baseline.",
    "impact_by_query_type": "Compares latency across different query categories. Slash Commands hit MongoDB directly (fast). Text Queries/Actions require LLM reasoning (slower). Multi-Turn queries include conversation context. Shows which query types benefit most from each optimization.",
    "llm_vs_tool": "Shows what percentage of total response time is spent on LLM thinking vs tool execution (MongoDB queries, embeddings). Demonstrates that LLM is the bottleneck (~96%), not the database. Tool time stays constant; optimizations reduce LLM time.",
    "token_savings": "Shows reduction in input tokens sent to the LLM for each query type. Fewer tokens = faster responses + lower API costs. Compression and streamlined prompts achieve 70-90% reduction by summarizing tool results and removing verbose instructions.",
    "operation_breakdown": "Shows how tool execution time is split between Embedding generation (Voyage API), MongoDB queries, and Python processing. Tool time stays consistent across all configs (~500ms), proving that context optimizations reduce LLM time, not database time."
}

# Intent-based grouping for comparison matrix
# Groups queries by what they accomplish, showing different input methods for the same task
INTENT_GROUPS = [
    {
        "intent": "List All Tasks",
        "icon": "üìã",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": 1},   # /tasks
            {"type": "text", "icon": "üí¨", "test_id": 11},   # What are my tasks?
            {"type": "voice", "icon": "üé§", "test_id": 34},  # Voice: What are my tasks?
        ]
    },
    {
        "intent": "Filter by Status (In Progress)",
        "icon": "üîÑ",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": 2},   # /tasks status:in_progress
            {"type": "text", "icon": "üí¨", "test_id": 12},   # What's in progress?
            {"type": "voice", "icon": "üé§", "test_id": 35},  # Voice version
        ]
    },
    {
        "intent": "Filter by Priority (High)",
        "icon": "üî•",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": 4},   # /tasks priority:high
            {"type": "text", "icon": "üí¨", "test_id": 13},   # Show high priority
            {"type": "voice", "icon": "üé§", "test_id": None},  # Not implemented
        ]
    },
    {
        "intent": "Show Project (AgentOps)",
        "icon": "üìÅ",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": 5},   # /tasks project:AgentOps
            {"type": "text", "icon": "üí¨", "test_id": 14},   # Show me the AgentOps project
            {"type": "voice", "icon": "üé§", "test_id": 36},  # Voice version
        ]
    },
    {
        "intent": "Show Project (Voice Agent)",
        "icon": "üìÅ",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": None},  # No direct slash command
            {"type": "text", "icon": "üí¨", "test_id": 15},   # What's in the Voice Agent project?
            {"type": "voice", "icon": "üé§", "test_id": None},  # Not implemented
        ]
    },
    {
        "intent": "Search Tasks (debugging)",
        "icon": "üîç",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": 6},   # /tasks search debugging
            {"type": "text", "icon": "üí¨", "test_id": 16},   # Find tasks about debugging
            {"type": "voice", "icon": "üé§", "test_id": 37},  # Voice version
        ]
    },
    {
        "intent": "Search Tasks (memory)",
        "icon": "üîç",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": 10},  # /projects search memory
            {"type": "text", "icon": "üí¨", "test_id": 17},   # Search for memory-related tasks
            {"type": "voice", "icon": "üé§", "test_id": None},  # Not implemented
        ]
    },
    {
        "intent": "Complete Task",
        "icon": "‚úÖ",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": None},  # No direct slash for complete
            {"type": "text", "icon": "üí¨", "test_id": 19},   # I finished the debugging doc
            {"type": "voice", "icon": "üé§", "test_id": 38},  # Voice version
        ]
    },
    {
        "intent": "Start Task",
        "icon": "‚ñ∂Ô∏è",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": None},  # No direct slash
            {"type": "text", "icon": "üí¨", "test_id": 23},   # Start working on the voice agent app
            {"type": "voice", "icon": "üé§", "test_id": None},  # Not implemented
        ]
    },
    {
        "intent": "Add Note to Task",
        "icon": "üìù",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": None},  # No direct slash
            {"type": "text", "icon": "üí¨", "test_id": 25},   # Add note to voice agent
            {"type": "voice", "icon": "üé§", "test_id": 40},  # Voice version
        ]
    },
    {
        "intent": "Multi-Turn: Context Recall",
        "icon": "üîÑ",
        "tests": [
            {"type": "slash", "icon": "‚ö°", "test_id": None},  # N/A for slash
            {"type": "text", "icon": "üí¨", "test_id": 30},   # What's high priority? (after AgentOps)
            {"type": "voice", "icon": "üé§", "test_id": None},  # Not implemented
        ]
    },
]


def init_session_state():
    """Initialize session state variables."""
    if "comparison_run" not in st.session_state:
        st.session_state.comparison_run = None
    if "selected_configs" not in st.session_state:
        st.session_state.selected_configs = DEFAULT_SELECTED.copy()
    if "coordinator" not in st.session_state:
        st.session_state.coordinator = None


def init_coordinator():
    """Initialize the coordinator if not already done."""
    if st.session_state.coordinator is None:
        try:
            st.session_state.coordinator = coordinator
        except Exception as e:
            st.error(f"Failed to initialize coordinator: {e}")


def render_config_section():
    """Render configuration selection."""
    st.subheader("üî¨ Test Configuration")

    st.write("**Select configurations to compare:**")

    cols = st.columns(len(EVAL_CONFIGS))
    selected = []

    for i, (key, config) in enumerate(EVAL_CONFIGS.items()):
        with cols[i]:
            checked = st.checkbox(
                config["short"],
                value=key in st.session_state.selected_configs,
                help=config["description"],
                key=f"cfg_{key}"
            )
            if checked:
                selected.append(key)

    st.session_state.selected_configs = selected

    # Action buttons
    col1, col2, col3, col4 = st.columns([1, 1, 1, 1])

    with col1:
        if st.button("üöÄ Run Comparison", type="primary", disabled=len(selected) < 2):
            st.session_state.run_comparison = True

    with col2:
        has_results = st.session_state.get("comparison_run") is not None
        if st.button("üì• Export JSON", disabled=not has_results):
            st.session_state.export_results = True

    with col3:
        if st.button("üóëÔ∏è Clear Results"):
            st.session_state.comparison_run = None
            st.rerun()

    with col4:
        if len(selected) < 2:
            st.caption("Select 2+ configs")
        else:
            st.caption(f"{len(selected)} configs selected")


def render_summary_section():
    """Render summary metrics cards comparing baseline to best."""
    st.subheader("üìà Summary")

    run: ComparisonRun = st.session_state.get("comparison_run")
    if not run or not run.summary_by_config:
        st.info("No results yet. Select configs and click 'Run Comparison'.")
        return

    # Get baseline and best config summaries
    baseline = run.summary_by_config.get("baseline", {})

    # Find best config (lowest latency, excluding baseline)
    best_key = None
    best_latency = float('inf')
    for key, summary in run.summary_by_config.items():
        if key != "baseline" and summary.get("avg_latency_ms", float('inf')) < best_latency:
            best_latency = summary["avg_latency_ms"]
            best_key = key

    best = run.summary_by_config.get(best_key, {}) if best_key else {}

    if not baseline or not best:
        st.warning("Need baseline and at least one other config for comparison.")
        return

    # Calculate improvements
    def calc_change(baseline_val, best_val):
        if baseline_val == 0:
            return 0
        return round((best_val - baseline_val) / baseline_val * 100, 1)

    latency_change = calc_change(baseline.get("avg_latency_ms", 0), best.get("avg_latency_ms", 0))
    tokens_change = calc_change(baseline.get("avg_tokens_in", 0), best.get("avg_tokens_in", 0))

    st.caption(f"Comparing: **Baseline** ‚Üí **{EVAL_CONFIGS.get(best_key, {}).get('name', best_key)}**")

    # Metric cards
    col1, col2, col3, col4, col5 = st.columns(5)

    with col1:
        st.metric(
            "Avg Latency",
            f"{best.get('avg_latency_ms', 0) / 1000:.1f}s",
            f"{latency_change:+.0f}%",
            delta_color="inverse",  # Red for positive (slower), green for negative (faster)
            help=METRIC_EXPLANATIONS["avg_latency"]
        )

    with col2:
        st.metric(
            "Avg Tokens In",
            f"{best.get('avg_tokens_in', 0):,}",
            f"{tokens_change:+.0f}%",
            delta_color="inverse",  # Lower is better
            help=METRIC_EXPLANATIONS["avg_tokens_in"]
        )

    with col3:
        st.metric(
            "Avg Tokens Out",
            f"{best.get('avg_tokens_out', 0):,}",
            help=METRIC_EXPLANATIONS["avg_tokens_out"]
        )

    with col4:
        accuracy = best.get("pass_rate", 0) * 100
        baseline_accuracy = baseline.get("pass_rate", 0) * 100
        acc_change = accuracy - baseline_accuracy
        st.metric(
            "Accuracy",
            f"{accuracy:.0f}%",
            f"{acc_change:+.0f}%",
            delta_color="normal",  # Higher is better
            help=METRIC_EXPLANATIONS["accuracy"]
        )

    with col5:
        total = best.get("total_tests", 0)
        passed = int(total * best.get("pass_rate", 0))
        st.metric(
            "Pass Rate",
            f"{passed}/{total}",
            help=METRIC_EXPLANATIONS["pass_rate"]
        )


def format_metric_value(value, metric_field):
    """Format metric value for display."""
    if value is None:
        return "-"

    if metric_field in ["latency_ms", "llm_time_ms", "tool_time_ms"]:
        # Time metrics - show in ms or s
        if value >= 1000:
            return f"{value/1000:.1f}s"
        else:
            return f"{value:.0f}ms"
    elif metric_field in ["tokens_in", "tokens_out"]:
        # Token metrics - show with comma separator
        return f"{value:,.0f}"
    else:
        return str(value)


def render_matrix_section():
    """Render comparison matrix grouped by task intent."""
    st.subheader("üß™ Comparison Matrix")
    st.caption("Grouped by task intent, showing different input methods for the same goal")

    run: ComparisonRun = st.session_state.get("comparison_run")
    if not run or not run.tests:
        st.info("Matrix will appear here after running comparison.")
        return

    # Metric selector toggle
    metric_options = ["Latency", "Tokens In", "Tokens Out", "LLM Time", "Tool Time"]
    selected_metric_name = st.radio(
        "Select metric to display:",
        options=metric_options,
        horizontal=True,
        key="matrix_metric_selector"
    )

    # Map display name to field name
    metric_field_map = {
        "Latency": "latency_ms",
        "Tokens In": "tokens_in",
        "Tokens Out": "tokens_out",
        "LLM Time": "llm_time_ms",
        "Tool Time": "tool_time_ms"
    }
    selected_metric = metric_field_map[selected_metric_name]

    # Build test lookup
    tests_by_id = {t.test_id: t for t in run.tests}

    # Config order
    config_order = ["baseline", "compress_results", "streamlined_prompt", "prompt_caching", "all_context"]
    configs = [c for c in config_order if c in run.configs_compared]

    # Render each intent group
    for group in INTENT_GROUPS:
        with st.expander(f"{group['icon']} {group['intent']}", expanded=False):
            # Header row
            header_cols = st.columns([0.4, 0.8, 2.5] + [1.2] * len(configs) + [1.5])
            header_cols[0].markdown("**#**")
            header_cols[1].markdown("**Type**")
            header_cols[2].markdown("**Query**")
            for i, config in enumerate(configs):
                if config == "baseline":
                    header_cols[3 + i].markdown(f"**{EVAL_CONFIGS[config]['short']}**")
                else:
                    # Add asterisk for optimization columns with tooltip
                    header_cols[3 + i].markdown(
                        f"**{EVAL_CONFIGS[config]['short']}**<sup>*</sup>",
                        unsafe_allow_html=True,
                        help="LLM optimization - no effect on slash commands (direct DB queries)"
                    )
            header_cols[-1].markdown("**Best**")

            st.markdown("---")

            # Test rows within this intent group
            for test_info in group["tests"]:
                test_id = test_info["test_id"]

                if test_id is None:
                    # Not implemented yet - show placeholder row
                    cols = st.columns([0.4, 0.8, 2.5] + [1.2] * len(configs) + [1.5])
                    cols[0].write("-")
                    cols[1].write(f"{test_info['icon']} {test_info['type'].title()}")
                    cols[2].write("*(coming soon)*")
                    for i in range(len(configs)):
                        cols[3 + i].write("-")
                    cols[-1].write("-")
                    continue

                test = tests_by_id.get(test_id)
                if not test:
                    continue

                # Render test row with selected metric
                render_matrix_row(test, configs, test_info, selected_metric)

    # Add explanatory footnote
    st.markdown("")  # Spacing
    st.caption(
        "<sup>*</sup> **LLM Optimizations** (Compress, Streamlined, Caching, All Ctx): "
        "These reduce LLM thinking time and token usage. They have no effect on slash commands, "
        "which query MongoDB directly without LLM processing. *Italic values* in optimization columns "
        "for slash commands show natural database variance, not optimization impact.",
        unsafe_allow_html=True
    )


def render_matrix_row(test, configs: list, test_info: dict, selected_metric: str):
    """Render a single row in the grouped matrix."""
    cols = st.columns([0.4, 0.8, 2.5] + [1.2] * len(configs) + [1.5])

    # Test ID
    cols[0].write(f"**{test.test_id}**")

    # Input type icon + label
    cols[1].write(f"{test_info['icon']} {test_info['type'].title()}")

    # Query (truncated)
    query_short = test.query[:30] + "..." if len(test.query) > 30 else test.query
    cols[2].write(query_short)

    # Get metric values and find best
    values = {}
    baseline_value = None
    best_config = None
    best_value = None

    for config in configs:
        result = test.results_by_config.get(config)
        value = getattr(result, selected_metric, None) if result else None
        values[config] = value

        if config == "baseline":
            baseline_value = value

        # For all metrics, lower is better (latency, tokens, time)
        if value is not None and (best_value is None or value < best_value):
            best_value = value
            best_config = config

    # Display metric values
    for i, config in enumerate(configs):
        value = values[config]
        formatted = format_metric_value(value, selected_metric)

        is_slash = test_info["type"] == "slash"
        is_optimization = config != "baseline"

        # Gray italic for slash commands in optimization columns (variation is just noise)
        if is_slash and is_optimization:
            cols[3 + i].markdown(
                f"<span style='color: #6b7280; font-style: italic;'>{formatted}</span>",
                unsafe_allow_html=True
            )
        # Normal display for slash baseline values
        elif is_slash:
            cols[3 + i].write(formatted)
        # Highlight best value for non-slash commands
        elif config == best_config and value is not None:
            cols[3 + i].markdown(f"üü¢ **{formatted}**")
        # Normal display for other values
        else:
            cols[3 + i].write(formatted)

    # Best column with improvement %
    # For slash commands, don't show "best" optimization - they bypass the LLM entirely
    if test_info["type"] == "slash":
        # Slash commands go directly to MongoDB, LLM optimizations don't apply
        cols[-1].write("‚ö° Direct DB")
    elif best_config and baseline_value and best_value and baseline_value > 0:
        improvement = ((baseline_value - best_value) / baseline_value) * 100
        best_name = EVAL_CONFIGS[best_config]["short"]
        cols[-1].write(f"‚úÖ {best_name} (-{improvement:.0f}%)")
    elif best_config:
        best_name = EVAL_CONFIGS[best_config]["short"]
        cols[-1].write(f"‚úÖ {best_name}")
    else:
        cols[-1].write("-")


def compute_section_averages(comparison_run: ComparisonRun) -> dict:
    """
    Aggregate test results by section for charts.

    Returns dict with per-section, per-config averages:
    {
        "slash_commands": {
            "baseline": {
                "avg_latency": 123.4,
                "avg_tokens_in": 1000,
                "avg_tokens_out": 50,
                "avg_llm_time": 100,
                "avg_tool_time": 23,
                "test_count": 10
            },
            "all_context": {...}
        },
        "text_queries": {...},
        ...
    }
    """
    from evals.test_suite import Section

    sections = [s.value for s in Section]
    results = {}

    for section in sections:
        # Get all tests for this section
        section_tests = [t for t in comparison_run.tests if t.section == section]
        results[section] = {}

        for config in comparison_run.configs_compared:
            # Get all results for this config in this section
            config_results = []
            for test in section_tests:
                if config in test.results_by_config:
                    config_results.append(test.results_by_config[config])

            if config_results:
                results[section][config] = {
                    "avg_latency": sum(r.latency_ms or 0 for r in config_results) / len(config_results),
                    "avg_tokens_in": sum(r.tokens_in or 0 for r in config_results) / len(config_results),
                    "avg_tokens_out": sum(r.tokens_out or 0 for r in config_results) / len(config_results),
                    "avg_llm_time": sum(r.llm_time_ms or 0 for r in config_results) / len(config_results),
                    "avg_tool_time": sum(r.tool_time_ms or 0 for r in config_results) / len(config_results),
                    "test_count": len(config_results)
                }

    return results


def compute_tool_usage(comparison_run: ComparisonRun) -> dict:
    """
    Aggregate tool usage across all tests.

    Returns dict with per-config tool statistics:
    {
        "baseline": {
            "get_tasks": {"count": 10, "avg_count_per_test": 0.25},
            "search_tasks": {"count": 5, "avg_count_per_test": 0.125},
            ...
        },
        "all_context": {...}
    }
    """
    tool_stats = {}

    for config in comparison_run.configs_compared:
        tool_stats[config] = {}

        for test in comparison_run.tests:
            result = test.results_by_config.get(config)
            if not result or not result.tools_called:
                continue

            for tool in result.tools_called:
                if tool not in tool_stats[config]:
                    tool_stats[config][tool] = {"count": 0}
                tool_stats[config][tool]["count"] += 1

        # Calculate average calls per test
        total_tests = len(comparison_run.tests) if comparison_run.tests else 1
        for tool in tool_stats[config]:
            count = tool_stats[config][tool]["count"]
            tool_stats[config][tool]["avg_count_per_test"] = count / total_tests

    return tool_stats


def compute_operation_breakdown(comparison_run: ComparisonRun) -> dict:
    """
    Aggregate operation timing breakdown across all tests.

    Returns dict with per-config average timing:
    {
        "baseline": {
            "embedding": 120,  # ms
            "mongodb": 85,     # ms
            "processing": 45,  # ms
            "llm": 2340        # ms
        },
        "all_context": {...}
    }
    """
    breakdown = {}

    for config in comparison_run.configs_compared:
        embedding_times = []
        mongodb_times = []
        processing_times = []
        llm_times = []

        for test in comparison_run.tests:
            result = test.results_by_config.get(config)
            if not result:
                continue

            if result.embedding_time_ms is not None and result.embedding_time_ms > 0:
                embedding_times.append(result.embedding_time_ms)
            if result.mongodb_time_ms is not None and result.mongodb_time_ms > 0:
                mongodb_times.append(result.mongodb_time_ms)
            if result.processing_time_ms is not None and result.processing_time_ms > 0:
                processing_times.append(result.processing_time_ms)
            if result.llm_time_ms is not None and result.llm_time_ms > 0:
                llm_times.append(result.llm_time_ms)

        breakdown[config] = {
            "embedding": round(sum(embedding_times) / len(embedding_times)) if embedding_times else 0,
            "mongodb": round(sum(mongodb_times) / len(mongodb_times)) if mongodb_times else 0,
            "processing": round(sum(processing_times) / len(processing_times)) if processing_times else 0,
            "llm": round(sum(llm_times) / len(llm_times)) if llm_times else 0
        }

    return breakdown


def render_charts_section():
    """Render impact analysis charts."""
    st.subheader("üìà Impact Analysis")

    run: ComparisonRun = st.session_state.get("comparison_run")
    if not run or not run.summary_by_config:
        st.info("Charts will appear here after running comparison.")
        return

    # Prepare data
    configs = run.configs_compared
    summaries = run.summary_by_config

    # Row 1: Optimization Waterfall and Impact by Query Type
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**‚ö° Optimization Waterfall**", help=CHART_EXPLANATIONS["waterfall"])
        render_optimization_waterfall(run)

    with col2:
        st.markdown("**üìä Impact by Query Type**", help=CHART_EXPLANATIONS["impact_by_query_type"])
        render_impact_by_query_type(run)

    # Row 2: LLM vs Tool Time and Token Savings
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**üîß LLM vs Tool Time**", help=CHART_EXPLANATIONS["llm_vs_tool"])
        render_llm_tool_breakdown(run)

    with col2:
        st.markdown("**ü™ô Token Savings by Query Type**", help=CHART_EXPLANATIONS["token_savings"])
        render_token_savings_by_type(run)

    # Row 3: Operation Time Breakdown
    st.markdown("**‚è±Ô∏è Tool Time Breakdown**", help=CHART_EXPLANATIONS["operation_breakdown"])
    render_operation_breakdown(run)


def render_optimization_waterfall(run: ComparisonRun):
    """Horizontal waterfall showing cumulative latency reduction."""
    # Order: baseline ‚Üí compress ‚Üí streamlined ‚Üí caching ‚Üí all_context
    waterfall_order = ["baseline", "compress_results", "streamlined_prompt", "prompt_caching", "all_context"]

    # Filter to only configs that were run
    ordered_configs = [cfg for cfg in waterfall_order if cfg in run.configs_compared]

    if len(ordered_configs) < 2:
        st.info("Run baseline + optimizations to see waterfall")
        return

    # Compute averages EXCLUDING slash commands (they bypass LLM, optimizations don't apply)
    # Only include text queries, actions, and multi-turn tests
    latencies = []
    labels = []
    for cfg in ordered_configs:
        # Filter out slash commands
        llm_tests = [t for t in run.tests if t.section != "slash_commands"]

        # Compute average for this config
        config_latencies = []
        for test in llm_tests:
            if cfg in test.results_by_config:
                result = test.results_by_config[cfg]
                if result.latency_ms:
                    config_latencies.append(result.latency_ms)

        avg_latency_ms = sum(config_latencies) / len(config_latencies) if config_latencies else 0
        latencies.append(avg_latency_ms / 1000)  # Convert to seconds
        labels.append(EVAL_CONFIGS[cfg]["short"])

    # Calculate % reduction from baseline
    baseline = latencies[0] if latencies else 1
    reductions = [f"{((baseline - lat) / baseline * 100):.0f}%" for lat in latencies]

    # Color gradient
    colors = ['#6b7280', '#8b5cf6', '#3b82f6', '#f59e0b', '#10b981'][:len(latencies)]

    # Create horizontal bar chart
    fig = go.Figure(go.Bar(
        x=latencies,
        y=labels,
        orientation='h',
        text=[f"{lat:.1f}s ({red})" for lat, red in zip(latencies, reductions)],
        textposition='outside',
        marker_color=colors
    ))

    fig.update_layout(
        xaxis_title="Latency (seconds)",
        yaxis=dict(autorange="reversed"),  # Baseline at top
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#e5e7eb"),
        height=350,
        margin=dict(t=20)  # Reduce top margin since no title
    )

    st.plotly_chart(fig, use_container_width=True)


def render_impact_by_query_type(run: ComparisonRun):
    """Grouped bar chart showing all optimization configs by query type."""
    from evals.test_suite import Section, SECTION_NAMES

    # Get aggregated data by section
    section_averages = compute_section_averages(run)

    # Build data for sections (exclude voice)
    sections = [Section.SLASH_COMMANDS, Section.TEXT_QUERIES, Section.TEXT_ACTIONS, Section.MULTI_TURN]
    section_names = [SECTION_NAMES.get(s, s.value) for s in sections]

    # Order configs consistently
    config_order = ["baseline", "compress_results", "streamlined_prompt", "prompt_caching", "all_context"]
    configs_to_show = [c for c in config_order if c in run.configs_compared]

    if not configs_to_show:
        st.info("No configs to display")
        return

    # Create grouped bar chart
    fig = go.Figure()

    for config in configs_to_show:
        latencies = []
        for section in sections:
            section_key = section.value
            section_data = section_averages.get(section_key, {})
            lat = section_data.get(config, {}).get("avg_latency", 0) / 1000
            latencies.append(lat)

        fig.add_trace(go.Bar(
            name=EVAL_CONFIGS[config]["short"],
            x=section_names,
            y=latencies,
            marker_color=CONFIG_COLORS.get(config, "#6b7280")
        ))

    fig.update_layout(
        yaxis_title="Avg Latency (seconds)",
        barmode='group',
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#e5e7eb"),
        height=350,
        margin=dict(t=20),  # Reduce top margin since no title
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="center",
            x=0.5
        )
    )

    st.plotly_chart(fig, use_container_width=True)


def render_llm_tool_breakdown(run: ComparisonRun):
    """Stacked horizontal bars showing LLM time vs Tool time - reinforces LLM bottleneck."""
    # Only show baseline and all_context for clarity
    configs = ["baseline", "all_context"]
    configs = [c for c in configs if c in run.configs_compared]

    if not configs:
        st.info("Need baseline and/or all_context configs")
        return

    summaries = run.summary_by_config
    fig = go.Figure()

    for config in configs:
        summary = summaries.get(config, {})
        llm_time = summary.get("avg_llm_time_ms", 0)
        tool_time = summary.get("avg_tool_time_ms", 0)
        total = llm_time + tool_time if (llm_time + tool_time) > 0 else 1

        llm_pct = (llm_time / total) * 100
        tool_pct = (tool_time / total) * 100

        config_name = EVAL_CONFIGS[config]["short"]

        # LLM time bar
        fig.add_trace(go.Bar(
            name="LLM Time" if config == configs[0] else None,
            y=[config_name],
            x=[llm_pct],
            orientation='h',
            marker_color='#3b82f6',
            text=f"LLM: {llm_pct:.0f}% ({llm_time/1000:.1f}s)",
            textposition='inside',
            showlegend=(config == configs[0])
        ))

        # Tool time bar
        fig.add_trace(go.Bar(
            name="Tool Time" if config == configs[0] else None,
            y=[config_name],
            x=[tool_pct],
            orientation='h',
            marker_color='#10b981',
            text=f"Tools: {tool_pct:.0f}%",
            textposition='inside',
            showlegend=(config == configs[0])
        ))

    fig.update_layout(
        xaxis_title="% of Total Time",
        barmode='stack',
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#e5e7eb"),
        height=250,
        margin=dict(t=20),  # Reduce top margin since no title
        yaxis=dict(autorange="reversed"),
        legend=dict(orientation="h", yanchor="bottom", y=1.02)
    )

    # Add annotation about MongoDB performance
    fig.add_annotation(
        text="üí° MongoDB averages <200ms. LLM is the bottleneck.",
        xref="paper", yref="paper",
        x=0.5, y=-0.15,
        showarrow=False,
        font=dict(size=11, color="#9ca3af")
    )

    st.plotly_chart(fig, use_container_width=True)


def render_token_savings_by_type(run: ComparisonRun):
    """Horizontal bars showing token reduction % by query type."""
    from evals.test_suite import Section, SECTION_NAMES

    if "baseline" not in run.configs_compared:
        st.info("Need baseline for comparison")
        return

    # Find best optimization config
    opt_config = None
    if "all_context" in run.configs_compared:
        opt_config = "all_context"
    else:
        for cfg in run.configs_compared:
            if cfg != "baseline":
                opt_config = cfg
                break

    if not opt_config:
        st.info("Need optimization config")
        return

    # Get aggregated data by section
    section_averages = compute_section_averages(run)

    # Only show text_queries, text_actions, multi_turn (skip slash_commands and voice)
    sections = [Section.TEXT_QUERIES, Section.TEXT_ACTIONS, Section.MULTI_TURN]
    section_names = [SECTION_NAMES.get(s, s.value) for s in sections]

    savings = []
    annotations = []

    for section in sections:
        section_key = section.value
        section_data = section_averages.get(section_key, {})

        baseline_tokens = section_data.get("baseline", {}).get("avg_tokens_in", 0)
        optimized_tokens = section_data.get(opt_config, {}).get("avg_tokens_in", 0)

        if baseline_tokens > 0:
            reduction_pct = ((baseline_tokens - optimized_tokens) / baseline_tokens) * 100
            savings.append(reduction_pct)
            annotations.append(f"{int(baseline_tokens)} ‚Üí {int(optimized_tokens)}")
        else:
            savings.append(0)
            annotations.append("N/A")

    # Create horizontal bar chart
    fig = go.Figure(go.Bar(
        x=savings,
        y=section_names,
        orientation='h',
        text=[f"{s:.0f}% ({a})" for s, a in zip(savings, annotations)],
        textposition='outside',
        marker_color='#f59e0b'
    ))

    fig.update_layout(
        xaxis_title="Reduction %",
        xaxis=dict(range=[0, max(savings) * 1.3] if savings and max(savings) > 0 else [0, 50]),
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#e5e7eb"),
        height=250,
        margin=dict(t=20),  # Reduce top margin since no title
        yaxis=dict(autorange="reversed")
    )

    st.plotly_chart(fig, use_container_width=True)


def render_operation_breakdown(run: ComparisonRun):
    """Stacked horizontal bar showing NON-LLM time breakdown for all configs."""
    # Get operation timing breakdown
    operation_stats = compute_operation_breakdown(run)

    if not operation_stats:
        st.info("No operation timing data available")
        return

    # Show all configs in order
    config_order = ["baseline", "compress_results", "streamlined_prompt", "prompt_caching", "all_context"]
    configs = [c for c in config_order if c in run.configs_compared and c in operation_stats]

    if not configs:
        st.info("No configs with operation timing data")
        return

    # Create stacked horizontal bar chart
    fig = go.Figure()

    # Only non-LLM operations (LLM is shown in LLM vs Tool Breakdown chart)
    operations = [
        ("embedding", "Embedding (Voyage API)", "#8b5cf6"),  # Purple
        ("mongodb", "MongoDB Queries", "#10b981"),            # Green
        ("processing", "Processing (Python)", "#3b82f6"),    # Blue
    ]

    for op_key, op_label, op_color in operations:
        values = [operation_stats[config].get(op_key, 0) for config in configs]

        fig.add_trace(go.Bar(
            name=op_label,
            x=values,
            y=[EVAL_CONFIGS[config]["short"] for config in configs],
            orientation='h',
            marker_color=op_color,
            text=[f"{v:.0f}ms" if v > 0 else "" for v in values],
            textposition='inside',
            textfont=dict(color='white')
        ))

    # Calculate totals for insight annotation
    totals = [sum([operation_stats[c].get(op, 0) for op, _, _ in operations]) for c in configs]
    avg_total = sum(totals) / len(totals) if totals else 0

    fig.update_layout(
        xaxis_title="Time (ms)",
        barmode='stack',
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#e5e7eb"),
        height=350,  # Taller for 5 bars
        margin=dict(t=20),  # Reduce top margin since no title
        legend=dict(orientation="h", yanchor="bottom", y=1.02, x=0.5, xanchor="center"),
        yaxis=dict(autorange="reversed"),
        annotations=[
            dict(
                text=f"üí° Tool time is consistent across configs (~{avg_total:.0f}ms) ‚Äî optimizations reduce LLM time, not DB time",
                xref="paper", yref="paper",
                x=0.5, y=-0.12,
                showarrow=False,
                font=dict(size=11, color="#9ca3af"),
                xanchor="center"
            )
        ]
    )

    st.plotly_chart(fig, use_container_width=True)


def run_comparison():
    """Execute comparison across selected configs."""
    configs = st.session_state.selected_configs
    coordinator = st.session_state.coordinator

    if not coordinator:
        st.error("Coordinator not initialized")
        return

    if len(configs) < 2:
        st.error("Select at least 2 configs")
        return

    progress = st.progress(0, text="Starting comparison...")

    def update_progress(current, total, message):
        progress.progress(current / total, text=message)

    runner = ComparisonRunner(coordinator, progress_callback=update_progress)

    try:
        run = runner.run_comparison(configs, skip_voice=True)
        st.session_state.comparison_run = run

        # Save to MongoDB
        progress.progress(1.0, text="Saving results...")
        try:
            save_comparison_run(run)
            st.success(f"Completed {len(run.tests)} tests. Saved as {run.run_id}")
        except Exception as e:
            st.warning(f"Results not saved to DB: {e}")
            st.success(f"Completed {len(run.tests)} tests (not persisted)")

        progress.empty()
        st.rerun()
    except Exception as e:
        progress.empty()
        st.error(f"Error: {e}")


def calculate_improvements(run: ComparisonRun) -> dict:
    """Calculate improvement percentages for export."""
    if "baseline" not in run.summary_by_config:
        return {}

    baseline = run.summary_by_config["baseline"]
    improvements = {}

    for cfg, summary in run.summary_by_config.items():
        if cfg == "baseline":
            continue

        baseline_latency = baseline.get("avg_latency_ms", 1)
        baseline_tokens = baseline.get("avg_tokens_in", 1)
        baseline_accuracy = baseline.get("pass_rate", 0)

        cfg_latency = summary.get("avg_latency_ms", 0)
        cfg_tokens = summary.get("avg_tokens_in", 0)
        cfg_accuracy = summary.get("pass_rate", 0)

        improvements[cfg] = {
            "latency_reduction_pct": round((baseline_latency - cfg_latency) / baseline_latency * 100, 1) if baseline_latency else 0,
            "tokens_reduction_pct": round((baseline_tokens - cfg_tokens) / baseline_tokens * 100, 1) if baseline_tokens else 0,
            "accuracy_change_pct": round((cfg_accuracy - baseline_accuracy) * 100, 1)
        }

    return improvements


def export_results():
    """Export comparison results to JSON."""
    run: ComparisonRun = st.session_state.get("comparison_run")

    if not run:
        st.warning("No results to export. Run a comparison first.")
        return

    # Convert to exportable format
    export_data = {
        "run_id": run.run_id,
        "timestamp": run.timestamp,
        "exported_at": datetime.utcnow().isoformat(),
        "configs_compared": run.configs_compared,
        "config_details": {
            k: EVAL_CONFIGS[k] for k in run.configs_compared
        },
        "summary_by_config": run.summary_by_config,
        "improvements": calculate_improvements(run),
        "tests": [t.to_dict() for t in run.tests]
    }

    # Trigger download
    filename = f"evals_{run.run_id}.json"

    st.download_button(
        label="üì• Download JSON",
        data=json.dumps(export_data, indent=2, default=str),
        file_name=filename,
        mime="application/json"
    )


def render_history_sidebar():
    """Render sidebar with past runs."""
    st.sidebar.header("üìú Past Runs")

    try:
        runs = list_comparison_runs(limit=10)
    except:
        runs = []

    if not runs:
        st.sidebar.caption("No saved runs yet")
        return

    for run in runs:
        run_id = run.get("run_id", "unknown")
        timestamp = run.get("timestamp", "")
        configs = run.get("configs_compared", [])

        # Format timestamp
        if isinstance(timestamp, str):
            display_time = timestamp[:16].replace("T", " ")
        else:
            display_time = timestamp.strftime("%Y-%m-%d %H:%M") if timestamp else ""

        # Summary
        summary = run.get("summary_by_config", {})
        baseline_latency = summary.get("baseline", {}).get("avg_latency_ms", 0)

        col1, col2 = st.sidebar.columns([3, 1])
        with col1:
            st.caption(f"{display_time}")
            st.caption(f"{len(configs)} configs")
        with col2:
            if st.button("Load", key=f"load_{run_id}"):
                load_saved_run(run_id)


def load_saved_run(run_id: str):
    """Load a saved run into session state."""
    try:
        doc = load_comparison_run(run_id)
        if doc:
            # Reconstruct ComparisonRun from dict
            run = ComparisonRun(
                run_id=doc.get("run_id"),
                timestamp=doc.get("timestamp"),
                configs_compared=doc.get("configs_compared", []),
            )
            run.summary_by_config = doc.get("summary_by_config", {})

            # Reconstruct tests from stored data
            run.tests = []
            for test_dict in doc.get("tests", []):
                test = TestComparison(
                    test_id=test_dict["test_id"],
                    query=test_dict["query"],
                    section=test_dict["section"],
                    input_type=test_dict["input_type"],
                    expected=test_dict["expected"],
                )
                test.best_config = test_dict.get("best_config")
                test.best_latency_ms = test_dict.get("best_latency_ms")
                test.improvement_pct = test_dict.get("improvement_pct")
                test.notes = test_dict.get("notes", "")

                # Reconstruct results_by_config
                for config_key, result_dict in test_dict.get("results_by_config", {}).items():
                    result = ConfigResult(
                        config_key=result_dict["config_key"],
                        latency_ms=result_dict.get("latency_ms", 0),
                        llm_time_ms=result_dict.get("llm_time_ms"),
                        tool_time_ms=result_dict.get("tool_time_ms"),
                        embedding_time_ms=result_dict.get("embedding_time_ms"),
                        mongodb_time_ms=result_dict.get("mongodb_time_ms"),
                        processing_time_ms=result_dict.get("processing_time_ms"),
                        tokens_in=result_dict.get("tokens_in"),
                        tokens_out=result_dict.get("tokens_out"),
                        cache_hit=result_dict.get("cache_hit", False),
                        tools_called=result_dict.get("tools_called", []),
                        response=result_dict.get("response", ""),
                        error=result_dict.get("error"),
                        result=result_dict.get("result", "pending"),
                        rating=result_dict.get("rating", 0)
                    )
                    test.results_by_config[config_key] = result

                run.tests.append(test)

            st.session_state.comparison_run = run
            st.session_state.selected_configs = doc.get("configs_compared", [])
            st.rerun()
    except Exception as e:
        st.sidebar.error(f"Failed to load: {e}")


def main():
    st.title("üìä Flow Companion Evals Dashboard")
    st.caption("Compare optimization configurations across test queries")

    init_session_state()
    init_coordinator()

    # Sidebar with history
    render_history_sidebar()

    # Main content
    render_config_section()
    st.markdown("---")
    render_summary_section()
    st.markdown("---")
    render_charts_section()
    st.markdown("---")
    render_matrix_section()

    # Handle Run Comparison
    if st.session_state.get("run_comparison"):
        st.session_state.run_comparison = False
        run_comparison()

    # Handle Export
    if st.session_state.get("export_results"):
        st.session_state.export_results = False
        export_results()


if __name__ == "__main__":
    main()
